{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a 2D Unet model\n",
    "\n",
    "In this model, we will demonstrate how to train a U-Net model from scratch. It is strongly encouraged to create a separate environment for this:\n",
    "\n",
    "```\n",
    "conda create -n pytorch-segmentation python=3.9\n",
    "conda activate pytorch-segmentation\n",
    "```\n",
    "In order to correctly install pytorch, find the matching command for your system on the [pytorch page](https://pytorch.org/get-started/locally/):\n",
    "\n",
    "![](./torch_get_started.png)\n",
    "\n",
    "After that is done, install some more packages:\n",
    "\n",
    "```\n",
    "mamba install scikit-image albumentations segmentation-models-pytorch pandas matplotlib torchmetrics tensorboard -c conda-forge\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'segmentation_models_pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCheckpoint\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msegmentation_models_pytorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msmp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'segmentation_models_pytorch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from skimage import io\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import yaml\n",
    "\n",
    "#import albumentations as albu\n",
    "import torch, torchmetrics\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "\n",
    "The first thing we have to do for Pytorch training, is tpo create a custom `Dataset` class for our dataset. This dataset object will then serve as a utility through which Pytorch can access and load all the data from the drive. During the training process we will iterate over our dataset, so the `Dataset` implementation needs to have two important member functions:\n",
    "\n",
    "* `__len__()`: The Dataloader needs to know how *many* samples there are.\n",
    "* `__getitem__()`: The Dataloader needs to be able to access the i-th sample out of our whole dataset.\n",
    "\n",
    "In this example, we will create a pandas dataframe with two columns `image` and `mask` and pass this to the Dataset. Working with dataframes is particularly easy because it allows us to easily split our data into a training and a validation cohort.\n",
    "\n",
    "*Note 1*: We will add the option for augmentation to the datafly. If we pass an augmentation function as an argument (which should accept parameters `image` and `mask` as inputs), the augmentations are applied on-the-fly. We will use the [albumentations package](https://albumentations.ai/) for this.\n",
    "*Note 2*: We will use a pretrained model. Such pretrained models are typically implemented for \"normal\" images, i.e. RGB images. Hence, the used model expects images to have the dimensions `[3, Y, X]`. Thus, if we work with grayscale images, we need to stack the single channel we are working with 3 times to create an artifical RGB image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, root, image_dir='images', label_dir='labels', augmentation=None):\n",
    "        self.root = root\n",
    "        self.augmentation = augmentation\n",
    "        \n",
    "        self.image_dir = os.path.join(root, image_dir)\n",
    "        self.label_dir = os.path.join(root, label_dir)\n",
    "        \n",
    "        # Assuming image and label filenames match, we can just use one list for pairing\n",
    "        filenames = sorted(os.listdir(self.image_dir))\n",
    "\n",
    "        # Creating a DataFrame to store paired images and labels\n",
    "        self.data = pd.DataFrame({\n",
    "            'image_filenames': [os.path.join(self.image_dir, fname) for fname in filenames],\n",
    "            'label_filenames': [os.path.join(self.label_dir, fname) for fname in filenames]\n",
    "        })\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        image_filepath = self.data['image_filenames'].iloc[i]\n",
    "        label_filepath = self.data['label_filenames'].iloc[i]\n",
    "\n",
    "        image = io.imread(image_filepath) / 255\n",
    "        mask = np.argmax(io.imread(label_filepath), axis=2)  # Assuming label images have multiple channels for classes\n",
    "\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image.astype(np.float32), mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        return image.astype(np.float32).transpose((2, 0, 1)), mask[None, :]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'data.zip' in os.listdir('.'):\n",
    "    r = requests.get(r'https://zenodo.org/record/7213527/files/HE_segmentation_data.zip')\n",
    "    with open(\"data.zip\", 'wb') as f:\n",
    "        f.write(r.content)\n",
    "        \n",
    "with zipfile.ZipFile('./data.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.abspath('./data')\n",
    "root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this opportunity to quickly check what the above-defined Dataset class does with this. For thhis, we create an instance of the Dataset class using the dataframe with the filenames of our training data. Then we'll try to get an arbitrary sample from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyDataset = Dataset(root=os.path.join(root))\n",
    "sample = MyDataset[120]\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(15, 15), sharex=True, sharey=True)\n",
    "axes[0].imshow(sample[0].transpose((1,2,0)))  # need to transpose for matplotlib RGB format\n",
    "axes[1].imshow(sample[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation\n",
    "\n",
    "For a typical segmentation job, it makes sense to augment the data to a certain degree. Albumentations allows to compose several augmentations together and luckily apply it to the image and the mask alike on-the fly. We will use the following augmentations for the training:\n",
    "* Vertical flip: Flip the image upside down in 50% of all calls\n",
    "* Horizontal flip: Same, but horizontal\n",
    "* Random Rotate: * Randomly rotate image and mask by 90 degrees in 50% of all calls\n",
    "* Random brightness/contrast: randomly change the brightness/contrast setting of the image in 20% of all calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train = albu.Compose([\n",
    "    albu.VerticalFlip(p=0.5),\n",
    "    albu.HorizontalFlip(p=0.5),\n",
    "    albu.RandomRotate90(p=0.5),\n",
    "    albu.RandomBrightnessContrast(p=0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test/validation split\n",
    "\n",
    "Last but not least, we need to create three subsets from our dataset, a training-, test- and validation-cohort. In every epoch of training (see below), the training process will look at all images in the training cohort and update our model based on this. The model is then applied to the images in the test cohort without updating the model. This is to see how well the method is currently performing. Finally, the model is appliedto the image data in the validation cohort to measure its performance on unknown data.  Scikit-learn provides the KFold strategy for problems like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = int(0.1 * len(MyDataset))\n",
    "train_size = int(0.8 * (len(MyDataset) - val_size))\n",
    "test_size = len(MyDataset) - val_size - train_size\n",
    "train_dataset, test_dataset, validation_dataset = torch.utils.data.random_split(MyDataset, [train_size, test_size, val_size])\n",
    "\n",
    "print('Samples in training set: ', len(train_dataset))\n",
    "print('Samples in testing set: ', len(test_dataset))\n",
    "print('Samples in validation set: ', len(validation_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation and preparation\n",
    "\n",
    "Next, we have to actually create an instance of a model which we will train for a number of epochs. This section will mostly set some parameters which are explained here.\n",
    "\n",
    "* `n_classes`: As desribed elsewhere, we are only trying to separate a background nd a foreground here. Hence, there are only 2 classes in our case.\n",
    "* `epochs`: In one epoch, the the Dataloader will go throough the entire dataset, update the layers and then check the net's performance in the test dataset. This is then repeated `epochs` times\n",
    "* `batch_size`: During the training process, multiple images are stitched together to a batch of images. This is done along a batch axis that is added to the image data. Typically, images are provided in  `[B, C, Y, X]` shape, with `B` being the batch dimension, `C` the channel dimension and `Y, X` being the actual image dimensions. If the `batch_size` is set too large, the images may not fit on the GPU anymore. \n",
    "*Note*: Images in a batch are usually batch-averaged! In other words, the pixel intensity values will be z-score normalized using the commmon mean and standard deviation of the entire batch. Making the batch too small can disrupt these running statistics.\n",
    "\n",
    "* `learning_rate`: How much the weights of every layer should be changed  in every training step. This is also referred to as the momentum of the training - see [here](https://twitter.com/marktenenholtz/status/1490309316347248646) for a nice explanation!\n",
    "* `num_workers`: How many CPU cores are allowed to be used to operate the dataloaders to feed the data to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "log_interval = 10\n",
    "n_classes = 3\n",
    "batch_size = 24\n",
    "learning_rate = 2e-5\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The loss\n",
    "\n",
    "An aspect of paramount importance is the used **loss function**. After all, deep learning is all about passing data through the network, evaluating the performance and then changing the weights accordingly. The loss function determines how exactlly performance is measured. Torch offers a few different implementations but you can basically implement any metric that compares two label images and calculates something like a degree of similarity. Something that is very commonly used (and thus being used here) is the `CrossEntropyLoss()` function, which calculates the cross-entropy of two label images. The [CrossEntropy ](https://en.wikipedia.org/wiki/Cross_entropy) is closely related to the [Mutual Information](https://en.wikipedia.org/wiki/Cross_entropy).\n",
    "\n",
    "Since the cross-entropy is a bit abstract to interpret, we will use a more intuitive measure to monitor the performance of our network in the validation cohort: The [Jaccard-coefficient](https://en.wikipedia.org/wiki/Jaccard_index). During the training we should observe that the cross-entropy in the training process goes down while the Jaccard-index should converge closer to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_train = CrossEntropyLoss()\n",
    "criterion_test = torchmetrics.functional.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        loss_fn=CrossEntropyLoss(),\n",
    "        loss_fn_test=torchmetrics.functional.accuracy,\n",
    "        n_classes=3,\n",
    "        learning_rate=2e-5):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # store some parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_fn = loss_fn\n",
    "        self.loss_fn_test = loss_fn_test\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.model = smp.Unet(\n",
    "            encoder_name='resnet50',\n",
    "            encoder_weights='imagenet',\n",
    "            classes=self.n_classes,\n",
    "            activation=None,\n",
    "        )\n",
    "\n",
    "        self.encoder = self.model.encoder\n",
    "        self.decoder = self.model.decoder\n",
    "\n",
    "        # log hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss_fn(y_hat, y.squeeze())\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss_fn(y_hat, y.squeeze())\n",
    "        loss_accuracy = self.loss_fn_test(y_hat.argmax(\n",
    "            axis=1), y.int().squeeze(), average=None, num_classes=3, task=\"multiclass\")\n",
    "\n",
    "        log = {'accuracy {}'.format(i): loss_accuracy[i] for i in range(self.n_classes)}\n",
    "        log[\"validation_loss\"] = loss\n",
    "        self.log_dict(log)\n",
    "\n",
    "        # make a matplotlib figure of the prediction if batch index is zero\n",
    "        if batch_idx == 0:\n",
    "            fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "            axes[0].imshow(x[0, 0, :, :].cpu().numpy())\n",
    "            axes[1].imshow(y[0, 0, :, :].cpu().numpy())\n",
    "            axes[2].imshow(y_hat.argmax(axis=1)[0, :, :].cpu().numpy())\n",
    "            self.logger.experiment.add_figure(\n",
    "                \"validation prediction\", fig, self.current_epoch)\n",
    "            \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now we have everything at hand to actually start training! For this, we first create Datasets from our train/test dataframes. Let's not forget to pass the composed albumentations **only to the training dataset.** We could also apply the augmentations to the trainign data, but it is preferable to have performance statistics on the real, unchanged image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "num_workers = 0\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "validation_dataloader = DataLoader(dataset=validation_dataset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every training epoch follows the following steps:\n",
    "\n",
    "* Set the model to training mode: In this mode, torch automatically updates the gradients of the model's layers on-the-fly as image data is passed through the layers. The optimizer can then have a look at these gradients to know how the convolutions in the respective layers need to be changed to improve performance\n",
    "* Reset the gradients known to the optimizer from the previous epoch\n",
    "* Pass each batch of training data through the network and calculate the loss (deviation of acquired result and correct mask)\n",
    "* Back-propagate the loss through the network and calculate the gradients\n",
    "* Let the optimizer update the weights of the network\n",
    "\n",
    "To check the progress of the training, navigate to the working directory in a terminal and open the tensorboard with\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then, navigate to http://localhost:6006/ in the browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "model.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfolder = Path(\"lightning_outputs\")\n",
    "if not logfolder.is_dir():\n",
    "    logfolder.mkdir()\n",
    "\n",
    "ckpt_callback = ModelCheckpoint(\n",
    "    filename='{epoch:03.0f}-{train_loss:.3f}',\n",
    "    save_last=True,\n",
    "    save_top_k=1,\n",
    "    monitor=\"train_loss\",\n",
    "    every_n_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "ndevices = torch.cuda.device_count()\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=logfolder,\n",
    "    max_epochs=epochs,\n",
    "    log_every_n_steps=log_interval,\n",
    "    accelerator=\"gpu\" if use_cuda else \"cpu\",\n",
    "    devices=ndevices if use_cuda else 1,\n",
    "    num_sanity_val_steps=0,\n",
    "    callbacks=[\n",
    "        ckpt_callback\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=validation_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Last but not least, let's apply the trained model to the data in the validation cohort and calculate performance statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel.load_from_checkpoint(\"./lightning_outputs/lightning_logs/version_10/checkpoints/epoch=098-train_loss=0.082.ckpt\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(validation_dataset[0][0][None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(torch.tensor(validation_dataset[0][0][None, :]).to('cuda'))\n",
    "prediction = prediction.detach().cpu().numpy()[0].transpose((1, 2, 0))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].imshow(validation_dataset[0][0].transpose((1,2,0)))\n",
    "axes[1].imshow(np.argmax(prediction, axis=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced implementation aspects:\n",
    "\n",
    "There are a few options we can to improve the training process and make it more robust. Some of them are listed here and will be expored in more advanced notebooks for the sake of this notebook's simplicity.\n",
    "\n",
    "* **Early Stopping:** We have implemented a naive way of stopping the training process if the model is good enough. However, to effectively prevent overfitting, we need to stop the training process as soon as performance is not improving anymore. Thus, a more suitable early stopping implementation would have to look at the train/test performance scores within the last X epochs and check if the variance of performance scores has been small. If so, the training process will be interrupted.\n",
    "* **Weighted sampling**: If we want to segmented regions in the image, which are rare in our training data, any well-behaving network should lean towards not predicting such labels *at all*. After all, the error in prediction is small if these labels are sufficiently scarce. To counter this, we can introduce [weighted sampling](https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler) to ensure that the network is equally exposed to all present labels in the image data.\n",
    "* **Scheduling**: As the model converges closer to its optimum, it is not wise to keep updating the model at the same speed as during the first epochs. We can do this by changing the `learning_rate` parameter as we progress through the epochs. Thus, the steps become smaller and safer. See [this tweet](https://twitter.com/marktenenholtz/status/1490309316347248646) for a nice visualization.\n",
    "* **Visualization**: It is good practice to visualize the training process through a side-by-side comparison of reference annotation and predicted label image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Applying the model to data is also called inference. Let's try this on some sample data. If you have not trained a model of your own previously, you can load the one that comes with the downloaded data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tk0 = tqdm.tqdm(valid_dataloader, total=len(valid_dataloader))\n",
    "    for b_idx, data in enumerate(tk0):\n",
    "\n",
    "        # Move images and masks in batch to GPU\n",
    "        for key, value in data.items():\n",
    "            data[key] = value.to(device).float()\n",
    "\n",
    "        # Feed the batch through the network and catch output into a new dictionary key\n",
    "        data['prediction']  = model(data['image'])\n",
    "\n",
    "        for i in range(data['image'].shape[0]):\n",
    "            fig, axes = plt.subplots(ncols=4, figsize=(10,40))\n",
    "            axes[0].imshow(data['image'][i].cpu().numpy().transpose((1,2,0)))\n",
    "            axes[1].imshow(data['prediction'][i][0].cpu().numpy())\n",
    "            axes[2].imshow(data['prediction'][i][1].cpu().numpy())\n",
    "            axes[3].imshow(data['prediction'][i][2].cpu().numpy())\n",
    "            axes[0].set_title('Raw')\n",
    "            axes[1].set_title('Background-ness')\n",
    "            axes[2].set_title('Necrosis-ness')\n",
    "            axes[3].set_title('Vital-ness')\n",
    "            fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = list(tk0)[-1]\n",
    "sample['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_children[0](sample['image'].float())\n",
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols=10\n",
    "fig, axes = plt.subplots(nrows=5, ncols=ncols, figsize=(15,8))\n",
    "\n",
    "batch = 0\n",
    "for k in range(1,6):\n",
    "    for i in range(ncols):\n",
    "        feature_maps = output[k][batch].detach().cpu().numpy()\n",
    "        axes[k-1, i].imshow(feature_maps[i], cmap='gray')\n",
    "        axes[k-1,0].set_ylabel('$N_{featuremaps}$ '+f'= {len(feature_maps)}' + '\\n' + f'{feature_maps[i].shape[0]} x {feature_maps[i].shape[1]}')\n",
    "fig.tight_layout()\n",
    "fig.savefig('featuremaps.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample['image'].detach().numpy()[batch].transpose((1,2,0)))\n",
    "plt.savefig('input.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devbio-napari_pol-course-pytorch",
   "language": "python",
   "name": "devbio-napari_pol-course-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
