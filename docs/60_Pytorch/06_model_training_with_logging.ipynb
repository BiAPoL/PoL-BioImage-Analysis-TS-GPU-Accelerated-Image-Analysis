{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with logging\n",
    "\n",
    "Training of the model in the previous notebook was leveraging the GPU in case it was available. However, after training and closing of this notebook, information about the course of training and the development of the loss was gone. We would like to keep this information as it might be relevant for diagnostic purposes later on, such as convergence or overfitting.\n",
    "\n",
    "A separate tool, originally developed within the tensorflow ecosystem but now adapted to pytorch, provides a solution for this: Tensorboard. More information is available [here](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_present = torch.cuda.is_available()\n",
    "ndevices = torch.cuda.device_count()\n",
    "use_cuda = cuda_present and ndevices > 0\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")  # \"cuda:0\" ... default device, \"cuda:1\" would be GPU index 1, \"cuda:2\" etc\n",
    "print(\"number of devices:\", ndevices, \"\\tchosen device:\", device, \"\\tuse_cuda=\", use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data import DSBData, get_dsb2018_train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets import BasicUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_files, train_lbl_files = get_dsb2018_train_files()\n",
    "\n",
    "train_data = DSBData(\n",
    "    image_files=train_img_files,\n",
    "    label_files=train_lbl_files,\n",
    "    target_shape=(256, 256)\n",
    ")\n",
    "\n",
    "print(len(train_data))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicUNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    features=[16, 16, 32, 64, 128, 16],\n",
    "    act=\"relu\",\n",
    "    norm=\"batch\",\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "# transfer the model to the chosen device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of a neural network means updating its parameters (weights) using a strategy that involves the gradients of a loss function with respect to the model parameters in order to adjust model weights to minimize this loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.e-3)\n",
    "init_params = list(model.parameters())[0].clone().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a training is performed by iterating over the batches of the training dataset multiple times. Each full iteration over the dataset is termed an epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**During or after training**, the tensorboard logs (which have been collected with the `SummaryWriter` object) can be visualized. Would you be on your laptop or workstation at home, you could do:\n",
    "\n",
    "```shell\n",
    "tensorboard --logdir \"path/to/logs\",\n",
    "```\n",
    "\n",
    "then open a browser using the URL `localhost:6006` (or whichever port the tensorboard server outputted as running on).\n",
    "Alternatively, tensorboard can be accessed from jupyter as well:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Taurus, some special steps need to be taken to visualize the tensorboard logs.\n",
    "\n",
    "If not done already, spawn a notebook BUT this time make sure to choose `production` under software environment in the advanced spawn configuration. Then wait until the notebooks open. Run this notebook.\n",
    "\n",
    "In order to be able to view the tensorboard logs, the tensorboard jupyter lab extension always checks the same location on the computer it is running on. Hence, you need to move your logs in the right location. To do so, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /tmp/$USER/tf-logs \n",
    "!ln -s $PWD/logs /tmp/$USER/tf-logs #might fail if the destination already exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the following cell which performs the model training. While the training runs, you can open the Tensorboad tab from the jupyter lab main page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_nepochs = 1\n",
    "log_interval = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"logs\", comment=\"this is the test of SummaryWriter\")\n",
    "\n",
    "model.train(True)\n",
    "\n",
    "# expects raw unnormalized scores and combines sigmoid + BCELoss for better\n",
    "# numerical stability.\n",
    "# expects B x C x W x D\n",
    "loss_function = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "for epoch in range(1, max_nepochs + 1):\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        # the inputs and labels have to be on the same device as the model\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        prediction_logits = model(X)\n",
    "        \n",
    "        batch_loss = loss_function(prediction_logits, y)\n",
    "\n",
    "        batch_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch:\",\n",
    "                epoch,\n",
    "                \"Batch:\",\n",
    "                batch_idx,\n",
    "                \"Total samples processed:\",\n",
    "                (batch_idx + 1) * train_loader.batch_size,\n",
    "                \"Loss:\",\n",
    "                batch_loss.item(),\n",
    "            )\n",
    "            writer.add_scalar(\"Loss/train\", batch_loss.item(), batch_idx)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you executed the cell above, you should see a new folder appear in the current directory. This folder is called `logs`. This is where tensorboard stores all run information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Let's do this locally\n",
    "\n",
    "The nice thing with having all the logs available on disk is, that you can move them around. If you like, try to download the entire `logs` folder onto your local machine (laptop). Then install `tensorboard` with pip or conda.\n",
    "\n",
    "```\n",
    "pip install tensorboard #can take awhile\n",
    "```\n",
    "\n",
    "Then run the same code as above on your local machine.\n",
    "```\n",
    "tensorboard --port 6006 --logdir /local/path/logs\n",
    "```\n",
    "You can now open a browser window and type in `localhost:6006` as URL. This should open the tensorboard interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devbio-napari_pol-course-pytorch",
   "language": "python",
   "name": "devbio-napari_pol-course-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
