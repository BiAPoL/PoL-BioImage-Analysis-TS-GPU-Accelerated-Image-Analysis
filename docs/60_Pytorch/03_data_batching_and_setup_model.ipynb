{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ebe65a",
   "metadata": {},
   "source": [
    "# Processing batches of data\n",
    "\n",
    "As most deep learning workflows benefit greatly from running on machines with GPUs that can process data in parallel, during model training the data is passed in batches of samples to the network instead of processing each sample sequentially. Torch offers great support for this which builds on top of a provided dataset.\n",
    "For convenience, the dataset class introduced in the previous notebook is part of the data module and we can now easily import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee2d544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import DSBData, get_dsb2018_train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd1a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_files, train_lbl_files = get_dsb2018_train_files()\n",
    "\n",
    "train_data = DSBData(\n",
    "    image_files=train_img_files,\n",
    "    label_files=train_lbl_files,\n",
    "    target_shape=(256, 256)\n",
    ")\n",
    "\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ebf6a7-5af9-4c82-b759-59f8805b2d72",
   "metadata": {},
   "source": [
    "Before starting to work with the data and actual models, we have to wrap out dataset object in a `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04df710-aa06-4985-9c8c-dcabd744dc71",
   "metadata": {},
   "source": [
    "The `DataLoader` comes with out of the box support for iterators that make looping code a bit more concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d57f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (batch_images, batch_labels) in enumerate(train_loader):\n",
    "    print(\"Batch\", batch_idx, batch_images.shape, batch_labels.shape)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90408d6",
   "metadata": {},
   "source": [
    "## Neural network architecture\n",
    "\n",
    "For semantic segmentation problems, a specific convolutional neural network architecture, i.e. a defined sequence of operations (also called layers) involving convolutional filters, data aggregation via pooling and nonlinear activation functions, has been demonstrated to work well across a wide range of image domains. This architecture is called UNet and its basic structure is shown below. (Image taken from [here](https://github.com/HarisIqbal88/PlotNeuralNet/blob/master/examples/Unet_Ushape/Unet_ushape.pdf).)\n",
    "\n",
    "<img src=\"unet.png\" alt=\"Drawing\" style=\"height: 400px;\"/>\n",
    "\n",
    "As this is rather cumbersome to implement directly, we will use the [MONAI](https://monai.io/) library, which provides a convenient torch implementation of this architecture by the name of `BasicUNet`. \n",
    "\n",
    "If you are interested, the [MONAI](https://monai.io/) library offers many more architectures in their [network architectures](https://docs.monai.io/en/stable/networks.html) documentation section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ba935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from monai.networks.nets import BasicUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51be9e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "BasicUNet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c56869",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicUNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    features=[16, 16, 32, 64, 128, 16],\n",
    "    act=\"relu\",\n",
    "    norm=\"batch\",\n",
    "    dropout=0.25,\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b3d662",
   "metadata": {},
   "source": [
    "We can now feed a batch of images directly through the model to obtain predictions. Note however, that those will likely not be usable for segmentation as the model has not been trained yet and model parameters are initialized randomly.\n",
    "\n",
    "Very importantly, the model outputs are of the same shape as the model inputs. Because the UNet consists entirely of convolutional operations, it is (to a degree) shape invariant and can process arbitrary input sizes. It is however recommended to work with resolutions that are divisible by 16, as the input resolution is halved in each of the four downsampling blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c2d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_preds = model(batch_images)\n",
    "print(batch_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a3e74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(131)\n",
    "plt.imshow(batch_images[0, 0].numpy(), cmap=\"gray\")\n",
    "plt.title(\"Input\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.imshow(batch_labels[0, 0].numpy(), cmap=\"gray\")\n",
    "plt.title(\"Ground truth\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.imshow(batch_preds.detach()[0, 0].numpy(), cmap=\"gray\")\n",
    "plt.title(\"Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4165f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different sized dummy input should be processable as well\n",
    "dummy_batch = torch.zeros(8, 1, 512, 512)\n",
    "dummy_preds = model(dummy_batch)\n",
    "print(dummy_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f252db93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different sized dummy input that is not divisible by 16, still produces output of same shape\n",
    "dummy_batch = torch.zeros(8, 1, 114, 87)\n",
    "dummy_preds = model(dummy_batch)\n",
    "print(dummy_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1469a93",
   "metadata": {},
   "source": [
    "The model output range is not limited to `[0,1.)` because in the output layer, no nonlinear activation was used which could have transformed the output pixel values as such. \n",
    "\n",
    "To fix this and make the output usable for segmentation purposes, we apply a [sigmoid activation](https://en.wikipedia.org/wiki/Sigmoid_function) function per pixel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8845d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_preds.min(), batch_preds.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8e883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_preds_seg = torch.nn.functional.sigmoid(batch_preds)\n",
    "print(batch_preds_seg.min(), batch_preds_seg.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5976c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(batch_preds_seg.detach()[0, 0], cmap=\"gray\")\n",
    "plt.colorbar(orientation=\"horizontal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588a98b",
   "metadata": {},
   "source": [
    "In order to obtain binary (0/1) predictions, a straightforward approach would be to use thresholding at 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9195de",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_preds_seg_binary = (batch_preds_seg > 0.5).to(torch.uint8)\n",
    "plt.imshow(batch_preds_seg_binary.detach()[0, 0], cmap=\"gray\")\n",
    "plt.colorbar(orientation=\"horizontal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81949bca-ad98-41f0-8d81-0a5b439b0fa0",
   "metadata": {},
   "source": [
    "Out model is not trained yet. So don't be bothered too much to just see garbage in the plot above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92837588-7f54-4b70-b821-ad1c37c68b92",
   "metadata": {},
   "source": [
    "## Exercise: My first MONAI BasicUnet\n",
    "\n",
    "Play with the model a bit. Take the constructor and change some parameters, e.g. the features, the activation, normalisation. Then, have the model predict on the same image as above. Display the prediction and compare to what we saw earlier. Do you spot a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ffcd79-3333-4c52-b99b-a84d4e40d41d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_intro_env",
   "language": "python",
   "name": "torch_intro_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
