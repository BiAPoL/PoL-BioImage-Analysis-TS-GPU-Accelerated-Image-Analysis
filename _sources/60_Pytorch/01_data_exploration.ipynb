{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "405c2b72-7d56-4ef3-b4c7-9f2a2dfa0bda",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26351dc-61ed-4a09-b318-fdf790b5e998",
   "metadata": {},
   "source": [
    "For this workshop, we will rely on a very simple dataset from the 2018 data science bowl. See [this page](https://bbbc.broadinstitute.org/BBBC038/) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870ffc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import get_dsb2018_train_files, get_dsb2018_validation_files, get_dsb2018_test_files, fill_label_holes, quantile_normalization\n",
    "from tifffile import imread\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f44606d",
   "metadata": {},
   "source": [
    "## Getting lists of input and label files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125cdc8b-c348-41e5-98ac-5e6976e69b1b",
   "metadata": {},
   "source": [
    "The data required to execute the notebooks is located at `/projects/p_scads_trainings/BIAS/dsb2018` and has to be integrated into your clone of this repository (which should reside in your home directory after clicking the above link to launch jupyter Hub). \n",
    "\n",
    "To get the data:  \n",
    "\n",
    "1. create a directory named `data` in the top level of this repo (i.e. on the same level the `*.ipynb*` notebook files and this README are located).  \n",
    "```\n",
    "mkdir data\n",
    "```\n",
    "\n",
    "\n",
    "2. copy the data to the freshly created directory using \n",
    "```\n",
    "cp -r /projects/p_scads_trainings/BIAS/dsb2018 $PWD/data/\n",
    "```\n",
    "\n",
    "\n",
    "As a backup solution, the data can be downloaded as a zip file from [the stardist github repository]('https://github.com/stardist/stardist/releases/download/0.1.0/dsb2018.zip')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc0b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's loop through the dataset and check how many samples we have\n",
    "for name, getter_fn in zip([\"train\", \"val\", \"test\"], [get_dsb2018_train_files, get_dsb2018_validation_files, get_dsb2018_test_files]):\n",
    "    X, y = getter_fn()\n",
    "    print(name, len(X), len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac8b46d-35ef-46b3-81e6-2f170e440e1a",
   "metadata": {},
   "source": [
    "We retain the last iteration of this loop, i.e. the test set. The variables `X` and `y` should contain paths to specific `.tif` files now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27ca9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b28f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cf0326",
   "metadata": {},
   "source": [
    "## Looking at a single sample of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f896730",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain = get_dsb2018_train_files()\n",
    "\n",
    "sidx = 0 #selecting the first image in the lists\n",
    "image_file, label_file = Xtrain[sidx], ytrain[sidx]\n",
    "image, label = imread(image_file), imread(label_file)\n",
    "label_filled = fill_label_holes(label) # some masks have holes, let's fill them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c445ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(image))\n",
    "print(type(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8f3a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, sample in zip([\"image\", \"label\"], [image, label]):\n",
    "    print(name, sample.dtype, sample.shape, sample.min(), sample.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a60fab1-5b49-472c-abdf-6b8e38cde8f9",
   "metadata": {},
   "source": [
    "The loaded images are 8-bit greyscale images. The labels however are encoded as 16-bit files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0889dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(131)\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.imshow(label)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.imshow(label_filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1245ebf9",
   "metadata": {},
   "source": [
    "## Convert the instance label to a binary segmentation mask\n",
    "\n",
    "As we intend to demonstrate the usage of pytorch, we are simplifying our problem from instance segmentation to semantic segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d2335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binary = np.zeros_like(label_filled)\n",
    "label_binary[label_filled != 0] = 1\n",
    "\n",
    "plt.imshow(label_binary, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629b2993",
   "metadata": {},
   "source": [
    "## Normalization of the raw image\n",
    "\n",
    "As neural networks tend to be easier to train when input values are small, we should normalize the pixel intensities from the uint8 range of [0, 255] to floating point values closer to [0, 1]. \n",
    "\n",
    "In the code below, we use a technique that sets the lower boundary of the normalization range to the 1% percentile. Equally, we set the upper boundary of the normalization to the 99.8%th percentile. This technique has proven to be very robust in practice. We adopted it from StarDist, see https://github.com/stardist/stardist/blob/master/examples/2D/2_training.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f622821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar normalization as shown in stardist (https://github.com/stardist/stardist/blob/master/examples/2D/2_training.ipynb)\n",
    "image_normalized_noclip = quantile_normalization(\n",
    "    image,\n",
    "    quantile_low=0.01,\n",
    "    quantile_high=0.998,\n",
    "    clip=False)[0]\n",
    "\n",
    "image_normalized_clip = quantile_normalization(\n",
    "    image,\n",
    "    quantile_low=0.01,\n",
    "    quantile_high=0.998,\n",
    "    clip=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370008f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"image intensity range before normalisation\")\n",
    "print(image_normalized_noclip.min(), image_normalized_noclip.max())\n",
    "\n",
    "print(\"image intensity range after normalisation\")\n",
    "print(image_normalized_clip.min(), image_normalized_clip.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4afd5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(131)\n",
    "_ = plt.hist(image.flatten(), density=True)\n",
    "\n",
    "plt.subplot(132)\n",
    "_ = plt.hist(image_normalized_noclip.flatten(), density=True)\n",
    "\n",
    "plt.subplot(133)\n",
    "_ = plt.hist(image_normalized_clip.flatten(), density=True)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab67b69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# a convenient transform from torchvision is to cast the \n",
    "# np.array to a torch.Tensor\n",
    "label_torch = transforms.ToTensor()(label_binary.astype(np.float32))\n",
    "\n",
    "# when using code that expects numpy objects, we have to cast back again\n",
    "plt.imshow(label_torch.numpy()[0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e6d126",
   "metadata": {},
   "source": [
    "## We explore the image resolutions on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efffa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's read in all training images\n",
    "X = list(map(imread, Xtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba41d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[1].shape, type(X[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a606d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = [tuple(x.shape) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dea082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will find many different shapes in the training data\n",
    "shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b299beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see the shapes we find\n",
    "unique_shapes = set(shapes)\n",
    "unique_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5027afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "for sh in unique_shapes:\n",
    "    counts[sh] = len([s for s in shapes if s == sh])\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5532475-260b-4f86-b778-fe1bf2ae0d4e",
   "metadata": {},
   "source": [
    "## Exercise: A homogenous dataset?\n",
    "\n",
    "If the shapes differ, what else is different. Explore the **training data** set more and find out:\n",
    "- are all images encoded the same way?\n",
    "- are all label masks encoded the same way?\n",
    "Once done, approach you the person next to you and discuss how you would proceed with such a diverse data set in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d950a2-37bb-4dc6-a512-53e745ea45d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_intro_env",
   "language": "python",
   "name": "torch_intro_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
