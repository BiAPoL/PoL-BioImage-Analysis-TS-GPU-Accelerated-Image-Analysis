{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c85a33d",
   "metadata": {},
   "source": [
    "# Creation of a dataset\n",
    "\n",
    "In deep learning, everything starts with a well-prepared dataset that provides inputs and outputs to the network that is supposed to be trained. Based on the data exploration of the previous notebook, we are creating a dataset class that can serve individual samples to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c6eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import get_dsb2018_train_files, get_dsb2018_validation_files, get_dsb2018_test_files, fill_label_holes, quantile_normalization\n",
    "from tifffile import imread\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5da2bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59838c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSBData():\n",
    "    def __init__(self, image_files, label_files, target_shape=(256, 256)):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        image_files: list of pathlib.Path objects pointing to the *.tif images\n",
    "        label_files: list of pathlib.Path objects pointing to the *.tif segmentation masks\n",
    "        target_shape: tuple of length 2 specifying the sample resolutions of files that\n",
    "                      will be kept. All other files will NOT be used.\n",
    "        \"\"\"\n",
    "        assert len(image_files) == len(label_files)\n",
    "        assert all(x.name==y.name for x,y in zip(image_files, label_files))\n",
    "\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        tensor_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        # use tqdm to have eye pleasing error bars\n",
    "        for idx in tqdm(range(len(image_files))):\n",
    "            # we use the same data reading approach as in the previous notebook\n",
    "            image = imread(image_files[idx])\n",
    "            label = imread(label_files[idx])\n",
    "\n",
    "            if image.shape != target_shape:\n",
    "                continue\n",
    "            \n",
    "            # do the normalizations\n",
    "            image = quantile_normalization(\n",
    "                image,\n",
    "                quantile_low=0.01,\n",
    "                quantile_high=0.998,\n",
    "                clip=True)[0].astype(np.float32)\n",
    "\n",
    "            # NOTE: we convert the label to dtype float32 and not uint8 because\n",
    "            # the tensor transformation does a normalization if the input is of\n",
    "            # dtype uint8, destroying the 0/1 labelling which we want to avoid.\n",
    "            label = fill_label_holes(label)\n",
    "            label_binary = np.zeros_like(label).astype(np.float32)\n",
    "            label_binary[label != 0] = 1.\n",
    "            \n",
    "            # convert to torch tensor: adds an artificial color channel in the front\n",
    "            # and scales inputs to have same size as samples tend to differ in image\n",
    "            # resolutions\n",
    "            image = tensor_transform(image)\n",
    "            label = tensor_transform(label_binary)\n",
    "\n",
    "            self.images.append(image)\n",
    "            self.labels.append(label)\n",
    "            \n",
    "        self.images = torch.stack(self.images)\n",
    "        self.labels = torch.stack(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30835205",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_files, train_lbl_files = get_dsb2018_train_files()\n",
    "\n",
    "n_samples = len(train_img_files)\n",
    "\n",
    "train_data = DSBData(\n",
    "    image_files=train_img_files[:n_samples],\n",
    "    label_files=train_lbl_files[:n_samples],\n",
    "    target_shape=(256, 256)\n",
    ")\n",
    "\n",
    "# NOTE: the length of the dataset might not be the same as n_samples\n",
    "#       because files not having the target shape will be discarded\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812b41a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.images.shape, train_data.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa29f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.images.min(), train_data.images.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9117de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.labels.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d0a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_files, val_lbl_files = get_dsb2018_validation_files()\n",
    "\n",
    "n_samples = len(val_img_files)\n",
    "\n",
    "val_data = DSBData(\n",
    "    image_files=val_img_files[:n_samples],\n",
    "    label_files=val_lbl_files[:n_samples],\n",
    "    target_shape=(256, 256)\n",
    ")\n",
    "\n",
    "# NOTE: the length of the dataset might not be the same as n_samples\n",
    "#       because files not having the target shape will be discarded\n",
    "print(len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75c3ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train_data[0]\n",
    "print(image.shape, label.shape)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(image[0].numpy(), cmap=\"gray\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(label[0].numpy(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136507e7-16d3-472c-bb8d-556ad87af74b",
   "metadata": {},
   "source": [
    "## Exercise: What if I'd chosen a different shape?\n",
    "\n",
    "Return to the last notebook. Check what different shapes are available in the data set. Compose a data set object only with them. Take 1-2 samples and display them in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73930e32-337c-45f4-ba77-a828668d1fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_intro_env",
   "language": "python",
   "name": "torch_intro_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
